{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classification"
      ],
      "metadata": {
        "id": "drsVx8spbEVq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 1: SKfold"
      ],
      "metadata": {
        "id": "EwoqpUDjlb8B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: with resampling\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(penalty='l2', max_iter=10000, random_state=0),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=0),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=0),\n",
        "    'SVM': SVC(probability=True, random_state=0),\n",
        "    'SVM2': SVC(kernel='sigmoid', probability=True, random_state=0),\n",
        "    'LightGBM': LGBMClassifier(random_state=0)\n",
        "}\n",
        "\n",
        "cv = RepeatedStratifiedKFold(n_splits=3, n_repeats=3, random_state=14)\n",
        "results = []\n",
        "fold_counter = 0\n",
        "\n",
        "for model_name, model in models.items():\n",
        "    train_aucs = []\n",
        "    val_aucs = []\n",
        "\n",
        "    for fold_idx, (train_idx, val_idx) in enumerate(cv.split(X_train, y_train)):\n",
        "        X_t, y_t = X_train.iloc[train_idx], y_train.iloc[train_idx]\n",
        "        X_val, y_val = X_train.iloc[val_idx], y_train.iloc[val_idx]\n",
        "\n",
        "        X_t_res, y_t_res = over.fit_resample(X_t, y_t)\n",
        "\n",
        "        model.fit(X_t_res, y_t_res)\n",
        "\n",
        "        if hasattr(model, \"predict_proba\"):\n",
        "            y_t_pred = model.predict_proba(X_t_res)[:, 1]\n",
        "            y_val_pred = model.predict_proba(X_val)[:, 1]\n",
        "        else:\n",
        "            y_t_pred = model.decision_function(X_t_res)\n",
        "            y_val_pred = model.decision_function(X_val)\n",
        "\n",
        "        train_auc = roc_auc_score(y_t_res, y_t_pred)\n",
        "        val_auc = roc_auc_score(y_val, y_val_pred)\n",
        "\n",
        "        train_aucs.append(train_auc)\n",
        "        val_aucs.append(val_auc)\n",
        "\n",
        "        fold_counter += 1\n",
        "        print(f\"{model_name} - Fold {fold_counter}: Train AUC = {train_auc:.4f}, Val AUC = {val_auc:.4f}\")\n",
        "\n",
        "    results.append({\n",
        "        'Model': model_name,\n",
        "        'Mean Train AUC': np.mean(train_aucs),\n",
        "        'Mean Validation AUC': np.mean(val_aucs)\n",
        "    })\n",
        "\n",
        "X_train_res, y_train_res = over.fit_resample(X_train, y_train)\n",
        "\n",
        "# After choosing the best model - SVM (RBF kernel)\n",
        "final_svm = SVC(probability=True, random_state=0)\n",
        "final_svm.fit(X_train_res, y_train_res)\n",
        "\n",
        "y_train_probs = final_svm.predict_proba(X_train_res)[:, 1]\n",
        "train_auc = roc_auc_score(y_train_res, y_train_probs)\n",
        "\n",
        "y_test_probs = final_svm.predict_proba(X_test)[:, 1]\n",
        "test_auc = roc_auc_score(y_test, y_test_probs)\n",
        "\n",
        "# Option 2: with/ no class weights\n",
        "\n",
        "# 2.1 With class weights\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(penalty='l2', class_weight='balanced', max_iter=10000, random_state=0),\n",
        "    'Decision Tree': DecisionTreeClassifier(class_weight='balanced', random_state=0),\n",
        "    'Random Forest': RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=0),\n",
        "    'SVM': SVC(class_weight='balanced', probability=True, random_state=0),\n",
        "    'SVM2': SVC(class_weight='balanced', kernel='sigmoid', probability=True, random_state=0),\n",
        "    'LightGBM': LGBMClassifier(is_unbalance=True, random_state=0)\n",
        "}\n",
        "\n",
        "\n",
        "# 2.2 No class weights\n",
        "\n",
        "models = {\n",
        "    'Logistic Regression': LogisticRegression(penalty='l2', max_iter=10000, random_state=0),\n",
        "    'Decision Tree': DecisionTreeClassifier(random_state=0),\n",
        "    'Random Forest': RandomForestClassifier(n_estimators=100, random_state=0),\n",
        "    'SVM': SVC(probability=True, random_state=0),\n",
        "    'SVM2': SVC(kernel='sigmoid', probability=True, random_state=0),\n",
        "    'LightGBM': LGBMClassifier(random_state=0)\n",
        "}\n",
        "\n",
        "results = []\n",
        "\n",
        "for repeat in range(3):\n",
        "    print(f\"\\nRepeat {repeat + 1}/3\")\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=repeat)\n",
        "\n",
        "    for model_name, model in models.items():\n",
        "        train_auc_scores = []\n",
        "        val_auc_scores = []\n",
        "\n",
        "        for fold, (train_idx, val_idx) in enumerate(skf.split(X_train, y_train)):\n",
        "            X_t, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
        "            y_t, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
        "\n",
        "            model.fit(X_t, y_t)\n",
        "            y_t_prob = model.predict_proba(X_t)[:, 1] if hasattr(model, \"predict_proba\") else model.decision_function(X_t)\n",
        "            y_val_prob = model.predict_proba(X_val)[:, 1] if hasattr(model, \"predict_proba\") else model.decision_function(X_val)\n",
        "\n",
        "            train_auc = roc_auc_score(y_t, y_t_prob)\n",
        "            val_auc = roc_auc_score(y_val, y_val_prob)\n",
        "\n",
        "            train_auc_scores.append(train_auc)\n",
        "            val_auc_scores.append(val_auc)\n",
        "\n",
        "            print(f\"  {model_name} - Fold {fold + 1}: Train AUC = {train_auc:.4f}, Val AUC = {val_auc:.4f}\")\n",
        "\n",
        "        results.append({\n",
        "            'Repeat': repeat + 1,\n",
        "            'Model': model_name,\n",
        "            'Mean Train AUC': np.mean(train_auc_scores),\n",
        "            'Mean Validation AUC': np.mean(val_auc_scores)\n",
        "        })\n",
        "\n",
        "\n",
        "# After choosing the best model\n",
        "\n",
        "# With class weights - SVM was the best\n",
        "final_svm = SVC(class_weight='balanced', probability=True, random_state=0)\n",
        "final_svm.fit(X_train, y_train)\n",
        "\n",
        "y_train_probs = final_svm.predict_proba(X_train)[:, 1]\n",
        "train_auc = roc_auc_score(y_train, y_train_probs)\n",
        "\n",
        "y_test_probs = final_svm.predict_proba(X_test)[:, 1]\n",
        "test_auc = roc_auc_score(y_test, y_test_probs)\n",
        "\n",
        "# Without class weights - Logistic Regression was the best\n",
        "final_lr = LogisticRegression(penalty='l2', max_iter=10000, random_state=0)\n",
        "final_lr.fit(X_train, y_train)\n",
        "\n",
        "y_train_probs = final_lr.predict_proba(X_train)[:, 1]\n",
        "train_auc = roc_auc_score(y_train, y_train_probs)\n",
        "\n",
        "y_test_probs = final_lr.predict_proba(X_test)[:, 1]\n",
        "test_auc = roc_auc_score(y_test, y_test_probs)"
      ],
      "metadata": {
        "id": "oDwhL4ybleiM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Method 2: Train/Dev/Test"
      ],
      "metadata": {
        "id": "BDbDrgFilUCy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Option 1: the models without class weights\n",
        "lr = LogisticRegression(penalty='l2',max_iter=10000, random_state=0)\n",
        "tree = DecisionTreeClassifier(random_state=0)\n",
        "rf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
        "svm = SVC(probability=True, random_state=0)\n",
        "svm2= SVC(kernel='sigmoid', probability=True, random_state=0)\n",
        "lgbm = LGBMClassifier(random_state=0)\n",
        "\n",
        "# Option 2: the models with class weights\n",
        "lr = LogisticRegression(penalty='l2', class_weight='balanced', max_iter=10000, random_state=0)\n",
        "tree = DecisionTreeClassifier(class_weight='balanced', random_state=0)\n",
        "rf = RandomForestClassifier(class_weight='balanced', n_estimators=100, random_state=0)\n",
        "svm = SVC(class_weight='balanced', probability=True, random_state=0)\n",
        "svm2= SVC(class_weight='balanced', kernel='sigmoid', probability=True, random_state=0)\n",
        "lgbm = LGBMClassifier(is_unbalance=True, random_state=0)\n",
        "\n",
        "# Option 1 also fits the resampling method\n",
        "\n",
        "# Fit all models\n",
        "lr.fit(x_train, y_train)\n",
        "tree.fit(x_train, y_train)\n",
        "rf.fit(x_train, y_train)\n",
        "svm.fit(x_train, y_train)\n",
        "svm2.fit(x_train, y_train)\n",
        "lgbm.fit(x_train, y_train)\n",
        "\n",
        "# Predict\n",
        "y_proba_lr_train = lr.predict_proba(x_train)\n",
        "y_proba_lr_dev = lr.predict_proba(x_dev)\n",
        "\n",
        "y_proba_tree_train = tree.predict_proba(x_train)\n",
        "y_proba_tree_dev = tree.predict_proba(x_dev)\n",
        "\n",
        "y_proba_rf_train = rf.predict_proba(x_train)\n",
        "y_proba_rf_dev = rf.predict_proba(x_dev)\n",
        "\n",
        "y_proba_svm_train = svm.predict_proba(x_train)\n",
        "y_proba_svm_dev = svm.predict_proba(x_dev)\n",
        "\n",
        "y_proba_svm2_train = svm2.predict_proba(x_train)\n",
        "y_proba_svm2_dev = svm2.predict_proba(x_dev)\n",
        "\n",
        "y_proba_lgbm_train = lgbm.predict_proba(x_train)\n",
        "y_proba_lgbm_dev = lgbm.predict_proba(x_dev)"
      ],
      "metadata": {
        "id": "LZ7tk-ENbHgD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = ['Logistic Regression', 'Decision Tree', 'Random Forest', 'SVM', 'SVM2', 'LightGBM']\n",
        "\n",
        "train_metrics = [\n",
        "    classificationMetrics(y_train, y_proba_lr_train),\n",
        "    classificationMetrics(y_train, y_proba_tree_train),\n",
        "    classificationMetrics(y_train, y_proba_rf_train),\n",
        "    classificationMetrics(y_train, y_proba_svm_train),\n",
        "    classificationMetrics(y_train, y_proba_svm2_train),\n",
        "    classificationMetrics(y_train, y_proba_lgbm_train)\n",
        "]\n",
        "\n",
        "dev_metrics = [\n",
        "    classificationMetrics(y_dev, y_proba_lr_dev),\n",
        "    classificationMetrics(y_dev, y_proba_tree_dev),\n",
        "    classificationMetrics(y_dev, y_proba_rf_dev),\n",
        "    classificationMetrics(y_dev, y_proba_svm_dev),\n",
        "    classificationMetrics(y_dev, y_proba_svm2_dev),\n",
        "    classificationMetrics(y_dev, y_proba_lgbm_dev)\n",
        "]\n",
        "\n",
        "table = {\n",
        "    'Model': models,\n",
        "    'Train AUC':       [m['AUC'] for m in train_metrics],\n",
        "    'Dev AUC':         [m['AUC'] for m in dev_metrics],\n",
        "    'Train F1':        [m['F1'] for m in train_metrics],\n",
        "    'Dev F1':          [m['F1'] for m in dev_metrics],\n",
        "    'Train Recall':    [m['Recall'] for m in train_metrics],\n",
        "    'Dev Recall':      [m['Recall'] for m in dev_metrics],\n",
        "    'Train Precision': [m['Precision'] for m in train_metrics],\n",
        "    'Dev Precision':   [m['Precision'] for m in dev_metrics],\n",
        "}"
      ],
      "metadata": {
        "id": "Z65VENAleQQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Choosing AUC\n",
        "model_table = pd.DataFrame(table, columns =[\"Model\" , \"Train AUC\" , \"Dev AUC\"])\n",
        "model_table['Train/Dev Ratio'] = model_table[\"Train AUC\"] / model_table[\"Dev AUC\"]"
      ],
      "metadata": {
        "id": "Svs1C2PxeZCo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Continuing with the best method and model - SVM2 (SVM with sigmoid kernel) + resampling\n",
        "y_proba_svm2_test = svm2.predict_proba(x_test)\n",
        "train_metrics_svm2 = classificationMetrics(y_train, y_proba_svm2_train)\n",
        "dev_metrics_svm2 = classificationMetrics(y_dev, y_proba_svm2_dev)\n",
        "test_metrics_svm2 = classificationMetrics(y_test, y_proba_svm2_test)\n",
        "\n",
        "# Hyperparameter tuning with GridSearchCV\n",
        "param_grid = [\n",
        "    {\n",
        "        'kernel': ['sigmoid'],\n",
        "        'C': [0.01, 0.1, 1, 2, 3],\n",
        "        'gamma': [1e-3, 1e-2, 0.1, 1],\n",
        "        'coef0': [0, 0.5, 1, 2]\n",
        "    }\n",
        "]\n",
        "\n",
        "cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=0)\n",
        "svm = SVC(probability=True, random_state=0)\n",
        "grid_search = GridSearchCV(svm, param_grid, scoring='roc_auc', cv=cv, n_jobs=-1, verbose=2)\n",
        "grid_search.fit(x_train, y_train)\n",
        "print(\"Best parameters:\", grid_search.best_params_)\n",
        "best_svm = grid_search.best_estimator_"
      ],
      "metadata": {
        "id": "1aPITXY7etxc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Importance"
      ],
      "metadata": {
        "id": "OcKDCZEef6ID"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "result = permutation_importance(\n",
        "    best_svm,\n",
        "    x_train,\n",
        "    y_train,\n",
        "    n_repeats=10,\n",
        "    scoring='roc_auc',\n",
        "    random_state=0\n",
        ")\n",
        "\n",
        "importances_df = pd.DataFrame({\n",
        "    'Feature': x.columns,\n",
        "    'Importance Mean': result.importances_mean,\n",
        "}).sort_values(by='Importance Mean', ascending=False)\n",
        "\n",
        "# Top 15 features\n",
        "top_n = 15\n",
        "top_features = importances_df.head(top_n)\n",
        "means = top_features['Importance Mean'] * 100"
      ],
      "metadata": {
        "id": "tgC9Bsjaf9Ry"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Confusion Matrix & AUC-ROC Curve"
      ],
      "metadata": {
        "id": "kcb-ZEXlgjMW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Confusion matrix\n",
        "y_pred = (final_y_proba_test >= 0.45).astype(int)\n",
        "cm = confusion_matrix(y_test, y_pred, labels=[0, 1])\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1])\n",
        "\n",
        "# AUC-ROC curve\n",
        "y_proba = final_y_proba_test\n",
        "auc_roc = roc_auc_score(y_test, y_proba)"
      ],
      "metadata": {
        "id": "OMtup1Lagk8_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regression"
      ],
      "metadata": {
        "id": "IhWRe49WgniS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Linear Regression\n",
        "lr_reg = LinearRegression()\n",
        "lr_reg.fit(X_train_reg, y_train_reg, sample_weight=weights_train)\n",
        "yhat_train_lr = lr_reg.predict(X_train_reg)\n",
        "yhat_dev_lr = lr_reg.predict(X_dev_reg)\n",
        "yhat_test_lr = lr_reg.predict(X_test_reg)\n",
        "\n",
        "# Ridge Regression\n",
        "ridge_reg = Ridge()\n",
        "ridge_reg.fit(X_train_reg, y_train_reg, sample_weight=weights_train)\n",
        "yhat_train_ridge = ridge_reg.predict(X_train_reg)\n",
        "yhat_dev_ridge = ridge_reg.predict(X_dev_reg)\n",
        "yhat_test_ridge = ridge_reg.predict(X_test_reg)\n",
        "\n",
        "# Random Forest\n",
        "rf_reg = RandomForestRegressor(n_estimators=200, max_depth=6, random_state=42)\n",
        "rf_reg.fit(X_train_reg, y_train_reg, sample_weight=weights_train)\n",
        "yhat_train_rf = rf_reg.predict(X_train_reg)\n",
        "yhat_dev_rf = rf_reg.predict(X_dev_reg)\n",
        "yhat_test_rf = rf_reg.predict(X_test_reg)\n",
        "\n",
        "# Gradient Boosting\n",
        "gb_reg = GradientBoostingRegressor(n_estimators=200, max_depth=4, learning_rate=0.1, random_state=42)\n",
        "gb_reg.fit(X_train_reg, y_train_reg, sample_weight=weights_train)\n",
        "yhat_train_gb = gb_reg.predict(X_train_reg)\n",
        "yhat_dev_gb = gb_reg.predict(X_dev_reg)\n",
        "yhat_test_gb = gb_reg.predict(X_test_reg)\n",
        "\n",
        "# The chosen metric results\n",
        "mae_results = pd.DataFrame([\n",
        "    {\n",
        "        'Model': 'Linear Regression',\n",
        "        'Train MAE': round(mean_absolute_error(y_train_reg, yhat_train_lr), 3),\n",
        "        'Dev MAE': round(mean_absolute_error(y_dev_reg, yhat_dev_lr), 3),\n",
        "        'Test MAE': round(mean_absolute_error(y_test_reg, yhat_test_lr), 3)\n",
        "    },\n",
        "    {\n",
        "        'Model': 'Ridge',\n",
        "        'Train MAE': round(mean_absolute_error(y_train_reg, yhat_train_ridge), 3),\n",
        "        'Dev MAE': round(mean_absolute_error(y_dev_reg, yhat_dev_ridge), 3),\n",
        "        'Test MAE': round(mean_absolute_error(y_test_reg, yhat_test_ridge), 3)\n",
        "    },\n",
        "    {\n",
        "        'Model': 'Random Forest',\n",
        "        'Train MAE': round(mean_absolute_error(y_train_reg, yhat_train_rf), 3),\n",
        "        'Dev MAE': round(mean_absolute_error(y_dev_reg, yhat_dev_rf), 3),\n",
        "        'Test MAE': round(mean_absolute_error(y_test_reg, yhat_test_rf), 3)\n",
        "    },\n",
        "    {\n",
        "        'Model': 'Gradient Boosting',\n",
        "        'Train MAE': round(mean_absolute_error(y_train_reg, yhat_train_gb), 3),\n",
        "        'Dev MAE': round(mean_absolute_error(y_dev_reg, yhat_dev_gb), 3),\n",
        "        'Test MAE': round(mean_absolute_error(y_test_reg, yhat_test_gb), 3)\n",
        "    }"
      ],
      "metadata": {
        "id": "uQfdIBBIgmzO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning for the best results\n",
        "\n",
        "\n",
        "# 1. Random Forest\n",
        "\n",
        "param_dist = {\n",
        "    'n_estimators': randint(200, 600),\n",
        "    'max_depth': [10, 12, 14, 16, 18],\n",
        "    'min_samples_split': randint(5, 20),\n",
        "    'min_samples_leaf': randint(3, 10),\n",
        "    'max_features': ['sqrt', 'log2', 0.3, 0.5, None]\n",
        "}\n",
        "\n",
        "random_search = RandomizedSearchCV(\n",
        "    estimator=RandomForestRegressor(random_state=0),\n",
        "    param_distributions=param_dist,\n",
        "    n_iter=30,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "random_search.fit(X_train_reg, y_train_reg, sample_weight=weights_train)\n",
        "\n",
        "best_params_rf = random_search.best_params_\n",
        "best_rf = RandomForestRegressor(**best_params_rf, random_state=42)\n",
        "best_rf.fit(X_train_reg, y_train_reg, sample_weight=weights_train)\n",
        "\n",
        "yhat_train_rf = best_rf.predict(X_train_reg)\n",
        "yhat_dev_rf = best_rf.predict(X_dev_reg)\n",
        "yhat_test_rf = best_rf.predict(X_test_reg)\n",
        "\n",
        "mae_train = mean_absolute_error(y_train_reg, yhat_train_rf)\n",
        "mae_dev = mean_absolute_error(y_dev_reg, yhat_dev_rf)\n",
        "mae_test = mean_absolute_error(y_test_reg, yhat_test_rf)\n",
        "\n",
        "\n",
        "rf_random_results = pd.DataFrame([\n",
        "    {\n",
        "        'Model': 'Random Search RF',\n",
        "        'Best Params': str(best_params_rf),\n",
        "        'Train MAE': round(mae_train, 3),\n",
        "        'Dev MAE': round(mae_dev, 3),\n",
        "        'Test MAE': round(mae_test, 3)\n",
        "    }\n",
        "])\n",
        "\n",
        "\n",
        "# 2. Gradient Boosting\n",
        "\n",
        "param_dist_gb = {\n",
        "    'n_estimators': randint(200, 400),\n",
        "    'learning_rate': uniform(0.05, 0.07),\n",
        "    'max_depth': [3, 4, 5],\n",
        "    'min_samples_split': randint(5, 20),\n",
        "    'min_samples_leaf': randint(5, 15),\n",
        "    'subsample': [0.7, 0.8, 0.9],\n",
        "    'max_features': ['sqrt', 'log2', None]\n",
        "}\n",
        "\n",
        "\n",
        "random_search_gb = RandomizedSearchCV(\n",
        "    estimator=GradientBoostingRegressor(random_state=0),\n",
        "    param_distributions=param_dist_gb,\n",
        "    n_iter=30,\n",
        "    scoring='neg_mean_absolute_error',\n",
        "    cv=3,\n",
        "    verbose=1,\n",
        "    n_jobs=-1,\n",
        "    random_state=42,\n",
        "    return_train_score=True\n",
        ")\n",
        "\n",
        "random_search_gb.fit(X_train_reg, y_train_reg, sample_weight=weights_train)\n",
        "best_params_gb = random_search_gb.best_params_\n",
        "best_gb = GradientBoostingRegressor(**best_params_gb, random_state=42)\n",
        "best_gb.fit(X_train_reg, y_train_reg, sample_weight=weights_train)\n",
        "\n",
        "yhat_train_gb = best_gb.predict(X_train_reg)\n",
        "yhat_dev_gb = best_gb.predict(X_dev_reg)\n",
        "yhat_test_gb = best_gb.predict(X_test_reg)\n",
        "\n",
        "mae_train_gb = mean_absolute_error(y_train_reg, yhat_train_gb)\n",
        "mae_dev_gb = mean_absolute_error(y_dev_reg, yhat_dev_gb)\n",
        "mae_test_gb = mean_absolute_error(y_test_reg, yhat_test_gb)\n",
        "\n",
        "# Save to DataFrame\n",
        "gb_random_results = pd.DataFrame([{\n",
        "    'Model': 'Random Search GB',\n",
        "    'Best Params': str(best_params_gb),\n",
        "    'Train MAE': round(mae_train_gb, 3),\n",
        "    'Dev MAE': round(mae_dev_gb, 3),\n",
        "    'Test MAE': round(mae_test_gb, 3)\n",
        "}])"
      ],
      "metadata": {
        "id": "StAL4og8hltL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Feature Importance - Random Forest was chosen as the best model\n",
        "importances = best_rf.feature_importances_\n",
        "feature_names = X_train_reg.columns\n",
        "\n",
        "feat_importance_df = pd.DataFrame({\n",
        "    'Feature': feature_names,\n",
        "    'Importance': importances\n",
        "}).sort_values(by='Importance', ascending=False)\n",
        "\n",
        "top_n = 15\n",
        "top_feat = feat_importance_df.head(top_n)\n",
        "\n",
        "# SHAP Feature Importance\n",
        "explainer = shap.TreeExplainer(best_rf)\n",
        "shap_values = explainer(X_test_reg)\n",
        "shap.summary_plot(shap_values, X_test_reg, plot_type=\"bar\")\n",
        "shap.summary_plot(shap_values, X_test_reg)\n",
        "shap_df = pd.DataFrame(shap_values.values, columns=shap_values.feature_names)\n",
        "top_combined = mean_signed.sort_values(ascending=False).head(top_n)\n",
        "features = top_combined.index[::-1]\n",
        "values = top_combined.values[::-1]"
      ],
      "metadata": {
        "id": "2FwtWvVtjBs3"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}