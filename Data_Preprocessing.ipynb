{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Data Preprocessing for ML\n",
        "\n",
        "# Dividing numeric and categorical variables\n",
        "num_vars = [var for var in df_cleaned.columns if df_cleaned[var].dtype in ['int64', 'float64']]\n",
        "cat_vars = [var for var in df_cleaned.columns if df_cleaned[var].dtype not in ['int64', 'float64']]"
      ],
      "metadata": {
        "id": "-UlRxrvvYtGo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Outlier Treatment\n"
      ],
      "metadata": {
        "id": "m6pFe59Dr4_5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def find_numeric_outliers(df_cleaned):\n",
        "    outliers = {}\n",
        "    for col in df_cleaned:\n",
        "      if df_cleaned[col].dtype in ['float64', 'int64']:\n",
        "        quartile_1, quartile_3 = df_cleaned[col].quantile([.25, .75])\n",
        "        interquartile_range = quartile_3 - quartile_1\n",
        "        lower_bound = quartile_1 - (1.5 * interquartile_range)\n",
        "        upper_bound = quartile_3 + (1.5 * interquartile_range)\n",
        "        outlier_rows = df_cleaned[(df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)]\n",
        "        outliers[col] = {\n",
        "          'lower_bound': lower_bound,\n",
        "          'upper_bound': upper_bound,\n",
        "          'outliers': outlier_rows[col].tolist()\n",
        "        }\n",
        "    return outliers\n",
        "\n",
        "def create_outlier_matrix(df_cleaned):\n",
        "    outlier_mat = np.zeros_like(df_cleaned)\n",
        "    for j ,var in enumerate(df_cleaned.columns):\n",
        "        if var in num_vars:\n",
        "            column = df_cleaned[var]\n",
        "            q1 = column.quantile(0.25)\n",
        "            q3 = column.quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            l1 = q1 - 1.5*iqr\n",
        "            l2 = q3 + 1.5*iqr\n",
        "            outlier_index = column[(column<l1) | (column>l2)].index\n",
        "            for i in outlier_index:\n",
        "                outlier_mat[i,j] = 1\n",
        "    return outlier_mat\n",
        "\n",
        "sns.heatmap(outlier_mat)\n",
        "\n",
        "# Removing time-based variables, some highly skewed features, and variables with clinically expected distributions.\n",
        "num_vars = [var for var in num_vars if var not in all_to_remove]\n",
        "\n",
        "# The Fisher method for two-tailed test\n",
        "def cocor(x1,y1, x2,y2):\n",
        "    xy1 = x1.corr(y1, method='spearman')\n",
        "    xy2 = x2.corr(y2, method='spearman')\n",
        "    n1 = len(x1)\n",
        "    n2 = len(x2)\n",
        "    xy_z = 0.5 * np.log((1 + xy1)/(1 - xy1))\n",
        "    ab_z = 0.5 * np.log((1 + xy2)/(1 - xy2))\n",
        "    if n2 is None:\n",
        "        n2 = n1\n",
        "    se_diff_r = np.sqrt(1/(n1 - 3) + 1/(n2 - 3))\n",
        "    diff = xy_z - ab_z\n",
        "    z = abs(diff / se_diff_r)\n",
        "    p = (1 - norm.cdf(z)) * 2\n",
        "    return z, p\n",
        "\n",
        "# Test correlations\n",
        "p_values_corr = []\n",
        "for j, col in enumerate(df_cleaned.columns):\n",
        "    if col in num_vars:\n",
        "        s = df_cleaned[col]\n",
        "        s_withot_outliers = s.filter(items = [i for i in range(df_cleaned.shape[0]) if outlier_mat[i,j] == 0])\n",
        "        _,p = cocor(s , df_cleaned.any_vte_result , s_withot_outliers , df_cleaned.any_vte_result)\n",
        "        print(col, '----', 'corr with outliers: ', s.corr(df_cleaned.any_vte_result), 'corr without outliers: ', s_withot_outliers.corr(df_cleaned.any_vte_result))\n",
        "        p_values_corr.append(p)\n",
        "\n",
        "# Testing distributions with Kolmogorov-Smirnov test (stats.kstest())\n",
        "p_values_dist = []\n",
        "for j, col in enumerate(df_cleaned.columns):\n",
        "    if col in num_vars:\n",
        "        s = df_cleaned[col]\n",
        "        s_without_outliers = s[outlier_mat[:, j] == 0]\n",
        "        _, p = stats.kstest(s, s_without_outliers)\n",
        "        p_values_dist.append(p)\n",
        "\n",
        "# Creating a decision table for outlier treatment (num_vars)\n",
        "decision_table['p-value dist'] = p_values_dist\n",
        "decision_table['distribution changed'] = ['yes' if p < 0.05 else 'no' for p in p_values_dist]\n",
        "def drop(df_cleaned):\n",
        "    if ((df_cleaned['correlation changed'] == 'yes') & (df_cleaned['distribution changed'] == 'yes')):\n",
        "        return 'no'\n",
        "    else:\n",
        "        return 'yes'\n",
        "decision_table['drop'] = decision_table.apply(drop, axis = 1)\n",
        "\n",
        "# Converting the \"yes\" features to np.nan\n",
        "vars_with_outliers_to_drop = decision_table[decision_table['drop'] == 'yes'].index\n",
        "for j, col in enumerate(df_cleaned.columns):\n",
        "    if col in vars_with_outliers_to_drop:\n",
        "        outlier_rows = [i for i in range(df_cleaned.shape[0]) if outlier_mat[i,j] == 1 ]\n",
        "        df_cleaned.iloc[outlier_rows,j] = np.nan\n",
        "\n",
        "# For highly skewed features features\n",
        "def replace_outliers_with_nan(series):\n",
        "    q1 = series.quantile(0.25)\n",
        "    q3 = series.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    return series.where((series >= lower_bound) & (series <= upper_bound), np.nan)\n",
        "\n",
        "for col in cols_to_clean:\n",
        "df_cleaned[col] = replace_outliers_with_nan(df_cleaned[col])"
      ],
      "metadata": {
        "id": "X-6tE2jWZGBq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A new Copy of the dataset\n",
        "df_vte = df_cleaned.copy()"
      ],
      "metadata": {
        "id": "yxMI06owaUn_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Creating the target outcome for the regression task\n",
        "df_vte[\"days_to_any_vte_result\"] = df_vte[[\n",
        "    \"days_to_any_vte_result\",\n",
        "    \"days_to_end_of_study\"\n",
        "]].min(axis=1)"
      ],
      "metadata": {
        "id": "8YmYhdwWjrAB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Categorical and numerical groups\n",
        "num_vars = [var for var in df_vte.columns if df_vte[var].dtype.name != 'category']\n",
        "cat_vars = [var for var in df_vte.columns if df_vte[var].dtype.name == 'category']"
      ],
      "metadata": {
        "id": "mEhYSdzfjtyA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Missing Value Handling\n"
      ],
      "metadata": {
        "id": "A5V8cW_2ryUM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculating the percentage\n",
        "missing_percentages = df_vte.isnull().sum() * 100 / len(df_vte)\n",
        "missing_percentages = missing_percentages[missing_percentages > 0]\n",
        "\n",
        "# A total of 39 variables were found to have missing values, all with more than 60% missingness\n",
        "# Of these, 34 were numeric lab results and 5 were categorical variables related to rehabilitation admissions\n",
        "# Due to manual handaling of these variables, only a few examples will be presented\n",
        "\n",
        "# Example 1\n",
        "def categorize_pulse(value):\n",
        "    if pd.isna(value):\n",
        "        return 0\n",
        "    elif value <= 59 or value > 100:\n",
        "        return 1  # Bradycardia/ Tachycardia\n",
        "    else:\n",
        "        return 2  # normal\n",
        "\n",
        "# Example 2\n",
        "def categorize_saturation(value):\n",
        "    if pd.isna(value):\n",
        "        return 0 # missing\n",
        "    else:\n",
        "        return 1  # not missing\n",
        "\n",
        "# Example 3\n",
        "def categorize_dbp(value):\n",
        "    if pd.isna(value):\n",
        "        return 0\n",
        "    elif value < 60:\n",
        "        return 1  # Low DBP\n",
        "    elif value <= 80:\n",
        "        return 2  # Normal DBP\n",
        "    else:\n",
        "        return 3  # High DBP\n",
        "\n",
        "# Example 4\n",
        "def categorize_hb(value):\n",
        "    if pd.isna(value):\n",
        "        return 0\n",
        "    elif value < 12:\n",
        "        return 1  # Low (possible anemia)\n",
        "    elif value <= 16:\n",
        "        return 2  # Normal\n",
        "    else:\n",
        "        return 3  # High\n",
        "\n",
        "# Example 5\n",
        "def categorize_hct(value):\n",
        "    if pd.isna(value):\n",
        "        return 0\n",
        "    elif value < 36 or value > 50:\n",
        "        return 1  # Abnormal - Low (possible anemia)/ High\n",
        "    else:\n",
        "        return 2  # Normal"
      ],
      "metadata": {
        "id": "TLvXUrT_j2-Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Encoding"
      ],
      "metadata": {
        "id": "ncq4rVXNpTZu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Categorical variables were encoded using a combination of binning, one-hot encoding, and ordinal encoding, depending on their type and distribution.**"
      ],
      "metadata": {
        "id": "5bVFeoxQpZOE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Binning Example:\n",
        "\n",
        "# Charlson score\n",
        "df_vte['charlson_comorbidity_index-charlson score total'] = df_vte['charlson_comorbidity_index-charlson score total'].apply(\n",
        "    lambda x: 2 if x >= 2 else x)\n",
        "\n",
        "df_vte['charlson_comorbidity_index-charlson score total'] = df_vte['charlson_comorbidity_index-charlson score total'].astype('category')\n",
        "\n",
        "# One-Hot Encoding Example:\n",
        "\n",
        "# Gender\n",
        "df_vte['gender_m'] = [1 if x == 'Male' else 0 for x in df_vte.gender]\n",
        "df_vte['gender_f'] = [1 if x == 'Female' else 0 for x in df_vte.gender]\n",
        "df_vte['gender_m'] = df_vte['gender_m'].astype('category')\n",
        "df_vte['gender_f'] = df_vte['gender_f'].astype('category')\n",
        "df_vte.drop('gender', axis=1, inplace=True)\n",
        "\n",
        "# Ordinal Encoding Example:\n",
        "\n",
        "# Socioeconomic score\n",
        "socioeconomic_order = [['no data', 'Very Low', 'Low', 'Medium', 'High', 'Very High']]\n",
        "encoder_socio = OrdinalEncoder(categories=socioeconomic_order)\n",
        "df_vte['socioeconomic score five level scale'] = encoder_socio.fit_transform(\n",
        "    df_vte[['socioeconomic score five level scale']]\n",
        ").astype(int)"
      ],
      "metadata": {
        "id": "_375YwQRpnLG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Regression model dataset\n",
        "df_vte_regression = df_vte.copy()"
      ],
      "metadata": {
        "id": "5c6z14xTqlza"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Selection"
      ],
      "metadata": {
        "id": "z-cO5hmEquz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Classification:\n",
        "\n",
        "p_values = []\n",
        "columns = list(df_vte.columns)\n",
        "columns.remove('any_vte_result')\n",
        "for var in columns:\n",
        "    if var in num_vars:\n",
        "        # For numeric variables, perform Mann-Whitney U test\n",
        "        x1 = df_vte.query('any_vte_result == 1')[var]\n",
        "        x2 = df_vte.query('any_vte_result == 0')[var]\n",
        "        _, p = stats.mannwhitneyu(x1, x2)\n",
        "    else:\n",
        "        # For categorical variables, perform Chi-square test\n",
        "        _, p, _, _ = stats.chi2_contingency(pd.crosstab(df_vte[var], df_vte.any_vte_result))\n",
        "\n",
        "    p_values.append(p)\n",
        "selection = pd.DataFrame(p_values, index=columns, columns=['p_value'])\n",
        "selection['keep'] = ['yes' if p <= 0.05 else 'no' for p in selection.p_value]\n",
        "\n",
        "cols_to_drop = selection.query(\"keep == 'no'\").index\n",
        "df_vte.drop(cols_to_drop, axis=1, inplace=True)\n",
        "\n",
        "# Regression:\n",
        "\n",
        "target = 'days_to_any_vte_result'\n",
        "num_vars_reg = [var for var in df_vte_regression.columns\n",
        "                if df_vte_regression[var].dtype.name != 'category' and var != target]\n",
        "\n",
        "cat_vars_reg = [var for var in df_vte_regression.columns\n",
        "                if df_vte_regression[var].dtype.name == 'category']\n",
        "p_values = []\n",
        "for var in num_vars_reg + cat_vars_reg:\n",
        "    if var == target:\n",
        "        continue\n",
        "    if var in num_vars_reg:\n",
        "        # Spearman correlation for numeric vs. continuous\n",
        "        coef, p = stats.spearmanr(df_vte_regression[var], df_vte_regression[target], nan_policy='omit')\n",
        "    else:\n",
        "        # Kruskalâ€“Wallis test for categorical vs. continuous\n",
        "        groups = [df_vte_regression[df_vte_regression[var] == level][target].dropna()\n",
        "                  for level in df_vte_regression[var].unique()]\n",
        "        _, p = stats.kruskal(*groups)\n",
        "\n",
        "    p_values.append((var, p))\n",
        "selection_reg = pd.DataFrame(p_values, columns=['variable', 'p_value'])\n",
        "selection_reg['keep'] = selection_reg['p_value'].apply(lambda x: 'yes' if x <= 0.05 else 'no')\n",
        "selection_reg.set_index('variable', inplace=True)\n",
        "cols_to_drop_reg = selection_reg.query(\"keep == 'no'\").index\n",
        "df_vte_regression.drop(cols_to_drop_reg, axis=1, inplace=True)"
      ],
      "metadata": {
        "id": "If2XuvJeqzr1"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
