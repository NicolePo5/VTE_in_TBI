{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iJpbbRpyJNdj"
      },
      "source": [
        "# Data Preprocessing for ML\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-UlRxrvvYtGo"
      },
      "outputs": [],
      "source": [
        "# Dividing numeric and categorical variables\n",
        "num_vars = [var for var in df_cleaned.columns if df_cleaned[var].dtype in ['int64', 'float64']]\n",
        "cat_vars = [var for var in df_cleaned.columns if df_cleaned[var].dtype not in ['int64', 'float64']]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6pFe59Dr4_5"
      },
      "source": [
        "### Outlier Treatment\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X-6tE2jWZGBq"
      },
      "outputs": [],
      "source": [
        "def find_numeric_outliers(df_cleaned):\n",
        "    outliers = {}\n",
        "    for col in df_cleaned:\n",
        "      if df_cleaned[col].dtype in ['float64', 'int64']:\n",
        "        quartile_1, quartile_3 = df_cleaned[col].quantile([.25, .75])\n",
        "        interquartile_range = quartile_3 - quartile_1\n",
        "        lower_bound = quartile_1 - (1.5 * interquartile_range)\n",
        "        upper_bound = quartile_3 + (1.5 * interquartile_range)\n",
        "        outlier_rows = df_cleaned[(df_cleaned[col] < lower_bound) | (df_cleaned[col] > upper_bound)]\n",
        "        outliers[col] = {\n",
        "          'lower_bound': lower_bound,\n",
        "          'upper_bound': upper_bound,\n",
        "          'outliers': outlier_rows[col].tolist()\n",
        "        }\n",
        "    return outliers\n",
        "\n",
        "def create_outlier_matrix(df_cleaned):\n",
        "    outlier_mat = np.zeros_like(df_cleaned)\n",
        "    for j ,var in enumerate(df_cleaned.columns):\n",
        "        if var in num_vars:\n",
        "            column = df_cleaned[var]\n",
        "            q1 = column.quantile(0.25)\n",
        "            q3 = column.quantile(0.75)\n",
        "            iqr = q3 - q1\n",
        "            l1 = q1 - 1.5*iqr\n",
        "            l2 = q3 + 1.5*iqr\n",
        "            outlier_index = column[(column<l1) | (column>l2)].index\n",
        "            for i in outlier_index:\n",
        "                outlier_mat[i,j] = 1\n",
        "    return outlier_mat\n",
        "\n",
        "sns.heatmap(outlier_mat)\n",
        "\n",
        "# Removing time-based variables, some highly skewed features, and variables with clinically expected distributions.\n",
        "num_vars = [var for var in num_vars if var not in all_to_remove]\n",
        "\n",
        "# The Fisher method for two-tailed test\n",
        "def cocor(x1,y1, x2,y2):\n",
        "    xy1 = x1.corr(y1, method='spearman')\n",
        "    xy2 = x2.corr(y2, method='spearman')\n",
        "    n1 = len(x1)\n",
        "    n2 = len(x2)\n",
        "    xy_z = 0.5 * np.log((1 + xy1)/(1 - xy1))\n",
        "    ab_z = 0.5 * np.log((1 + xy2)/(1 - xy2))\n",
        "    if n2 is None:\n",
        "        n2 = n1\n",
        "    se_diff_r = np.sqrt(1/(n1 - 3) + 1/(n2 - 3))\n",
        "    diff = xy_z - ab_z\n",
        "    z = abs(diff / se_diff_r)\n",
        "    p = (1 - norm.cdf(z)) * 2\n",
        "    return z, p\n",
        "\n",
        "# Test correlations\n",
        "p_values_corr = []\n",
        "for j, col in enumerate(df_cleaned.columns):\n",
        "    if col in num_vars:\n",
        "        s = df_cleaned[col]\n",
        "        s_withot_outliers = s.filter(items = [i for i in range(df_cleaned.shape[0]) if outlier_mat[i,j] == 0])\n",
        "        _,p = cocor(s , df_cleaned.any_vte_result , s_withot_outliers , df_cleaned.any_vte_result)\n",
        "        print(col, '----', 'corr with outliers: ', s.corr(df_cleaned.any_vte_result), 'corr without outliers: ', s_withot_outliers.corr(df_cleaned.any_vte_result))\n",
        "        p_values_corr.append(p)\n",
        "\n",
        "# Testing distributions with Kolmogorov-Smirnov test (stats.kstest())\n",
        "p_values_dist = []\n",
        "for j, col in enumerate(df_cleaned.columns):\n",
        "    if col in num_vars:\n",
        "        s = df_cleaned[col]\n",
        "        s_without_outliers = s[outlier_mat[:, j] == 0]\n",
        "        _, p = stats.kstest(s, s_without_outliers)\n",
        "        p_values_dist.append(p)\n",
        "\n",
        "# Creating a decision table for outlier treatment (num_vars)\n",
        "decision_table['p-value dist'] = p_values_dist\n",
        "decision_table['distribution changed'] = ['yes' if p < 0.05 else 'no' for p in p_values_dist]\n",
        "def drop(df_cleaned):\n",
        "    if ((df_cleaned['correlation changed'] == 'yes') & (df_cleaned['distribution changed'] == 'yes')):\n",
        "        return 'no'\n",
        "    else:\n",
        "        return 'yes'\n",
        "decision_table['drop'] = decision_table.apply(drop, axis = 1)\n",
        "\n",
        "# Converting the \"yes\" features to np.nan\n",
        "vars_with_outliers_to_drop = decision_table[decision_table['drop'] == 'yes'].index\n",
        "for j, col in enumerate(df_cleaned.columns):\n",
        "    if col in vars_with_outliers_to_drop:\n",
        "        outlier_rows = [i for i in range(df_cleaned.shape[0]) if outlier_mat[i,j] == 1 ]\n",
        "        df_cleaned.iloc[outlier_rows,j] = np.nan\n",
        "\n",
        "# For highly skewed features features\n",
        "def replace_outliers_with_nan(series):\n",
        "    q1 = series.quantile(0.25)\n",
        "    q3 = series.quantile(0.75)\n",
        "    iqr = q3 - q1\n",
        "    lower_bound = q1 - 1.5 * iqr\n",
        "    upper_bound = q3 + 1.5 * iqr\n",
        "    return series.where((series >= lower_bound) & (series <= upper_bound), np.nan)\n",
        "\n",
        "for col in cols_to_clean:\n",
        "df_cleaned[col] = replace_outliers_with_nan(df_cleaned[col])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yxMI06owaUn_"
      },
      "outputs": [],
      "source": [
        "# A new Copy of the dataset\n",
        "df_vte = df_cleaned.copy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YmYhdwWjrAB"
      },
      "outputs": [],
      "source": [
        "# Creating the target outcome for the regression task\n",
        "df_vte[\"days_to_any_vte_result\"] = df_cleaned[[\"days_to_vte\", \"days_to_pe\", \"days_to_dvt\"]].min(axis=1, skipna=True)\n",
        "df_vte[\"days_to_end_of_study\"] = 540\n",
        "\n",
        "df_vte[\"days_to_any_vte_result\"] = df_vte[[\n",
        "    \"days_to_any_vte_result\",\n",
        "    \"days_to_end_of_study\"\n",
        "]].min(axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mEhYSdzfjtyA"
      },
      "outputs": [],
      "source": [
        "# Categorical and numerical groups\n",
        "num_vars = [var for var in df_vte.columns if df_vte[var].dtype.name != 'category']\n",
        "cat_vars = [var for var in df_vte.columns if df_vte[var].dtype.name == 'category']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A5V8cW_2ryUM"
      },
      "source": [
        "### Missing Value Handling\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TLvXUrT_j2-Z"
      },
      "outputs": [],
      "source": [
        "# Calculating the percentage\n",
        "missing_percentages = df_vte.isnull().sum() * 100 / len(df_vte)\n",
        "missing_percentages = missing_percentages[missing_percentages > 0]\n",
        "\n",
        "# A total of 39 variables were found to have missing values, all with more than 60% missingness\n",
        "# Of these, 34 were numeric lab results and 5 were categorical variables related to rehabilitation admissions\n",
        "# Due to manual handaling of these variables, only a few examples will be presented\n",
        "\n",
        "# Example 1\n",
        "def categorize_pulse(value):\n",
        "    if pd.isna(value):\n",
        "        return 0\n",
        "    elif value <= 59 or value > 100:\n",
        "        return 1  # Bradycardia/ Tachycardia\n",
        "    else:\n",
        "        return 2  # normal\n",
        "\n",
        "# Example 2\n",
        "def categorize_saturation(value):\n",
        "    if pd.isna(value):\n",
        "        return 0 # missing\n",
        "    else:\n",
        "        return 1  # not missing\n",
        "\n",
        "# Example 3\n",
        "def categorize_dbp(value):\n",
        "    if pd.isna(value):\n",
        "        return 0\n",
        "    elif value < 60:\n",
        "        return 1  # Low DBP\n",
        "    elif value <= 80:\n",
        "        return 2  # Normal DBP\n",
        "    else:\n",
        "        return 3  # High DBP\n",
        "\n",
        "# Example 4\n",
        "def categorize_hb(value):\n",
        "    if pd.isna(value):\n",
        "        return 0\n",
        "    elif value < 12:\n",
        "        return 1  # Low (possible anemia)\n",
        "    elif value <= 16:\n",
        "        return 2  # Normal\n",
        "    else:\n",
        "        return 3  # High\n",
        "\n",
        "# Example 5\n",
        "def categorize_hct(value):\n",
        "    if pd.isna(value):\n",
        "        return 0\n",
        "    elif value < 36 or value > 50:\n",
        "        return 1  # Abnormal - Low (possible anemia)/ High\n",
        "    else:\n",
        "        return 2  # Normal"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ncq4rVXNpTZu"
      },
      "source": [
        "### Encoding"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5bVFeoxQpZOE"
      },
      "source": [
        "**Categorical variables were encoded using a combination of binning, one-hot encoding, and ordinal encoding, depending on their type and distribution.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_375YwQRpnLG"
      },
      "outputs": [],
      "source": [
        "# Binning Example:\n",
        "\n",
        "# Charlson score\n",
        "df_vte['charlson_comorbidity_index-charlson score total'] = df_vte['charlson_comorbidity_index-charlson score total'].apply(\n",
        "    lambda x: 2 if x >= 2 else x)\n",
        "\n",
        "df_vte['charlson_comorbidity_index-charlson score total'] = df_vte['charlson_comorbidity_index-charlson score total'].astype('category')\n",
        "\n",
        "# One-Hot Encoding Example:\n",
        "\n",
        "# Gender\n",
        "df_vte['gender_m'] = [1 if x == 'Male' else 0 for x in df_vte.gender]\n",
        "df_vte['gender_f'] = [1 if x == 'Female' else 0 for x in df_vte.gender]\n",
        "df_vte['gender_m'] = df_vte['gender_m'].astype('category')\n",
        "df_vte['gender_f'] = df_vte['gender_f'].astype('category')\n",
        "df_vte.drop('gender', axis=1, inplace=True)\n",
        "\n",
        "# Ordinal Encoding Example:\n",
        "\n",
        "# Socioeconomic score\n",
        "socioeconomic_order = [['no data', 'Very Low', 'Low', 'Medium', 'High', 'Very High']]\n",
        "encoder_socio = OrdinalEncoder(categories=socioeconomic_order)\n",
        "df_vte['socioeconomic score five level scale'] = encoder_socio.fit_transform(\n",
        "    df_vte[['socioeconomic score five level scale']]\n",
        ").astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5c6z14xTqlza"
      },
      "outputs": [],
      "source": [
        "# Regression model dataset\n",
        "df_vte_regression = df_vte.copy()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z-cO5hmEquz3"
      },
      "source": [
        "### Feature Selection"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "If2XuvJeqzr1"
      },
      "outputs": [],
      "source": [
        "# Classification:\n",
        "\n",
        "p_values = []\n",
        "columns = list(df_vte.columns)\n",
        "columns.remove('any_vte_result')\n",
        "for var in columns:\n",
        "    if var in num_vars:\n",
        "        # For numeric variables, perform Mann-Whitney U test\n",
        "        x1 = df_vte.query('any_vte_result == 1')[var]\n",
        "        x2 = df_vte.query('any_vte_result == 0')[var]\n",
        "        _, p = stats.mannwhitneyu(x1, x2)\n",
        "    else:\n",
        "        # For categorical variables, perform Chi-square test\n",
        "        _, p, _, _ = stats.chi2_contingency(pd.crosstab(df_vte[var], df_vte.any_vte_result))\n",
        "\n",
        "    p_values.append(p)\n",
        "selection = pd.DataFrame(p_values, index=columns, columns=['p_value'])\n",
        "selection['keep'] = ['yes' if p <= 0.05 else 'no' for p in selection.p_value]\n",
        "\n",
        "cols_to_drop = selection.query(\"keep == 'no'\").index\n",
        "df_vte.drop(cols_to_drop, axis=1, inplace=True)\n",
        "\n",
        "# Regression:\n",
        "target = 'days_to_any_vte_result'\n",
        "num_vars_reg = [var for var in df_vte_regression.columns\n",
        "                if df_vte_regression[var].dtype.name != 'category' and var != target]\n",
        "\n",
        "cat_vars_reg = [var for var in df_vte_regression.columns\n",
        "                if df_vte_regression[var].dtype.name == 'category']\n",
        "p_values = []\n",
        "for var in num_vars_reg + cat_vars_reg:\n",
        "    if var == target:\n",
        "        continue\n",
        "    if var in num_vars_reg:\n",
        "        # Spearman correlation for numeric vs. continuous\n",
        "        coef, p = stats.spearmanr(df_vte_regression[var], df_vte_regression[target], nan_policy='omit')\n",
        "    else:\n",
        "        # Kruskal–Wallis test for categorical vs. continuous\n",
        "        groups = [df_vte_regression[df_vte_regression[var] == level][target].dropna()\n",
        "                  for level in df_vte_regression[var].unique()]\n",
        "        _, p = stats.kruskal(*groups)\n",
        "\n",
        "    p_values.append((var, p))\n",
        "selection_reg = pd.DataFrame(p_values, columns=['variable', 'p_value'])\n",
        "selection_reg['keep'] = selection_reg['p_value'].apply(lambda x: 'yes' if x <= 0.05 else 'no')\n",
        "selection_reg.set_index('variable', inplace=True)\n",
        "cols_to_drop_reg = selection_reg.query(\"keep == 'no'\").index\n",
        "df_vte_regression.drop(cols_to_drop_reg, axis=1, inplace=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii-kJw9tLAnH"
      },
      "source": [
        "## Classification"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BHqOFWgNMbLP"
      },
      "source": [
        "### Data Division"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nvI4mi9rMd93"
      },
      "outputs": [],
      "source": [
        "# For train/dev/test\n",
        "x = df_vte.drop(['any_vte_result'], axis=1)\n",
        "y = df_vte['any_vte_result']\n",
        "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state = 14)\n",
        "x_train, x_dev, y_train, y_dev = train_test_split(x_train, y_train, test_size=0.2, random_state = 14)\n",
        "\n",
        "# For SKfold\n",
        "X = df_vte.drop(columns=['any_vte_result'])\n",
        "y = df_vte['any_vte_result']\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=14)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5nvDEJY8JS0s"
      },
      "source": [
        "### Scaling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YJ0rBG18JYPe"
      },
      "outputs": [],
      "source": [
        "scaler = StandardScaler()\n",
        "x_train = scaler.fit_transform(x_train)\n",
        "x_dev = scaler.transform(x_dev)\n",
        "x_test = scaler.transform(x_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3HKILwHoJ0b6"
      },
      "source": [
        "### Data Imbalance Methods"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DcuA9XYJ_tI"
      },
      "outputs": [],
      "source": [
        "# Resampling for train/dev/test method\n",
        "over = RandomOverSampler(sampling_strategy='all', random_state=14)\n",
        "under = RandomUnderSampler(sampling_strategy='majority', random_state=14)\n",
        "smote = SMOTE(random_state=14)\n",
        "smote_enn = SMOTEENN(random_state=14)\n",
        "train_over_x , train_over_y = over.fit_resample(x_train, y_train)\n",
        "train_under_x , train_under_y = under.fit_resample(x_train, y_train)\n",
        "train_smote_x , train_smote_y  = smote.fit_resample(x_train, y_train)\n",
        "train_smote_enn_x, train_smote_enn_y = smote_enn.fit_resample(x_train, y_train)\n",
        "\n",
        "# Resampling for SKfold\n",
        "sampling_methods = {\n",
        "    'Original': (X_train, y_train),\n",
        "    'Random Over': over.fit_resample(X_train, y_train),\n",
        "    'Random Under': under.fit_resample(X_train, y_train)\n",
        "#     'SMOTE': smote.fit_resample(X_train, y_train),\n",
        "#     'SMOTEENN': smote_enn.fit_resample(X_train, y_train)\n",
        "}\n",
        "\n",
        "for method_name, (X_samp, y_samp) in sampling_methods.items():\n",
        "    print(f\"\\n=== {method_name} ===\")\n",
        "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
        "    train_auc_scores = []\n",
        "    val_auc_scores = []\n",
        "\n",
        "    for train_idx, val_idx in skf.split(X_samp, y_samp):\n",
        "        X_t, X_val = X_samp.iloc[train_idx], X_samp.iloc[val_idx]\n",
        "        y_t, y_val = y_samp.iloc[train_idx], y_samp.iloc[val_idx]\n",
        "\n",
        "        model = LogisticRegression(penalty='l2', max_iter=10000, random_state=0)\n",
        "        model\n",
        "        model.fit(X_t, y_t)\n",
        "        y_t_prob = model.predict_proba(X_t)[:, 1]\n",
        "        y_val_prob = model.predict_proba(X_val)[:, 1]\n",
        "\n",
        "        train_auc = roc_auc_score(y_t, y_t_prob)\n",
        "        val_auc = roc_auc_score(y_val, y_val_prob)\n",
        "\n",
        "        train_auc_scores.append(train_auc)\n",
        "        val_auc_scores.append(val_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NQBoCqMiLhZt"
      },
      "source": [
        "### Metrics for Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tExjfgu-LmF2"
      },
      "outputs": [],
      "source": [
        "def classificationMetrics(y, y_proba):\n",
        "    if y_proba.ndim == 2:\n",
        "        y_proba = y_proba[:, 1]\n",
        "    y_pred = (y_proba >= 0.5).astype(int)\n",
        "    auc = metrics.roc_auc_score(y, y_proba)\n",
        "    f1 = metrics.f1_score(y, y_pred)\n",
        "    recall = metrics.recall_score(y, y_pred)\n",
        "    precision = metrics.precision_score(y, y_pred)\n",
        "\n",
        "    return {\n",
        "        'AUC': auc,\n",
        "        'F1': f1,\n",
        "        'Recall': recall,\n",
        "        'Precision': precision\n",
        "    }"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oMcBXogLDdm"
      },
      "source": [
        "## Regression"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UF1gIW8cLHCG"
      },
      "outputs": [],
      "source": [
        "# From 540 days to 77 weeks\n",
        "df_vte_regression['days_to_any_vte_result'] = (\n",
        "    df_vte_regression['days_to_any_vte_result'].astype(int) // 7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jcvUiPx5NQzW"
      },
      "source": [
        "### Data Division"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0-EM-81wNST_"
      },
      "outputs": [],
      "source": [
        "X_reg = df_vte_regression.drop(columns='days_to_any_vte_result')\n",
        "Y_reg = df_vte_regression['days_to_any_vte_result']\n",
        "X_train_dev_reg, X_test_reg, y_train_dev_reg, y_test_reg = train_test_split(X_reg, Y_reg, test_size=0.2, random_state=14)\n",
        "X_train_reg, X_dev_reg, y_train_reg, y_dev_reg = train_test_split(X_train_dev_reg, y_train_dev_reg, test_size=0.2, random_state=42)\n",
        "\n",
        "# Lower weights for week 77 (censored)\n",
        "weights_train = np.where(y_train_reg == 77, 0.1, 1.0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_jSb_KIrPXIY"
      },
      "source": [
        "### Metrics for modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3ob4gxUBPa25"
      },
      "outputs": [],
      "source": [
        "def regressionMetrics(y, yhat):\n",
        "    yhat_clipped = np.maximum(yhat, 0)\n",
        "    y_clipped = np.maximum(y, 0)\n",
        "\n",
        "    return {\n",
        "        'MAE': metrics.mean_absolute_error(y, yhat),\n",
        "        'RMSE': metrics.mean_squared_error(y, yhat, squared=False),\n",
        "        'MSLE': metrics.mean_squared_log_error(y_clipped, yhat_clipped),\n",
        "    }"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
